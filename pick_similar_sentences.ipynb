{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "with open('w2i_movie_names.json') as f:\n",
    "    w2i_movies = json.load(f)\n",
    "\n",
    "with open('i2w_movie_names.json') as f:\n",
    "    i2w_movies = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('movie_data_separate.pkl', 'rb') as f:\n",
    "    movie_data = pickle.load(f)\n",
    "    \n",
    "with open('neighbours.pkl', 'rb') as f:\n",
    "    neighbours = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('w2i_review_comments_fact.json') as f:\n",
    "        w2i_rpc = json.load(f)\n",
    "with open('i2w_review_comments_fact.json') as f:\n",
    "        i2w_rpc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20155"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i2w_rpc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i_rpc['unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(k, embeddings):\n",
    "    k += 1\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='kd_tree')\n",
    "    nbrs.fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_utterance_embedding(utterance, embed_dim, stopwords, w2i, trained_word_embeddings):\n",
    "    #obtain the average embedding for the whole utterance using the word embeddings\n",
    "    #learned in the movie embedding training\n",
    "    utterance_embedding = torch.zeros(embed_dim)\n",
    "    \n",
    "    count = 0\n",
    "    for w in utterance:\n",
    "        #skip stop words\n",
    "        if w in stop_words or w in ['<SOS>', '<EOS>']:\n",
    "            pass\n",
    "        elif w in w2i: #word in dictionary\n",
    "            #print('word',w)\n",
    "            word_em = trained_word_embeddings[w2i[w]]\n",
    "            utterance_embedding += word_em\n",
    "            count += 1\n",
    "        else:\n",
    "            word_em = trained_word_embeddings[w2i_rpc['unknown']] #unk\n",
    "            utterance_embedding += word_em\n",
    "            count += 1\n",
    "            \n",
    "    #print(utterance_embedding, utterance_embedding/count, count)\n",
    "    avg_utterances_embedding = utterance_embedding/count\n",
    "    \n",
    "    return avg_utterances_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt0061452\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-183e3adddda8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mutterance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'which'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'your'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'favourite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'character'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#speaker 1 utterance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_movie.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0msimilar_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar_movie_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2i_rpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2w_rpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutterance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-183e3adddda8>\u001b[0m in \u001b[0;36mget_similar_movie_responses\u001b[0;34m(movie_id, n, model, w2i, i2w, utterance, neighbours, stop_words)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_word_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mavg_utterance_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_average_utterance_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutterance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_word_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msimilar_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-963e1fccf089>\u001b[0m in \u001b[0;36mget_average_utterance_embedding\u001b[0;34m(utterance, embed_dim, stopwords, w2i, trained_word_embeddings)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutterance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#skip stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<SOS>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<EOS>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#word in dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "def get_similar_movie_responses(movie_id, n, model, w2i, i2w, utterance, neighbours, stop_words):\n",
    "    '''\n",
    "    w2i and i2w are vocabularies for plot-review-comments\n",
    "    utterance is a tokenized sentence of strings\n",
    "    returning a list of tokenized sentences of strings\n",
    "    '''\n",
    "    similar_movie_data = neighbours[movie_id] \n",
    "    similar_movie_id = similar_movie_data.imdb_id\n",
    "    print(similar_movie_id)\n",
    "    similar_movie_chat = similar_movie_data.chat\n",
    "    \n",
    "    trained_word_embeddings = model['model_state_dict']['word_embedding.weight']\n",
    "    #print(trained_word_embeddings.shape)\n",
    "    embed_dim = trained_word_embeddings.shape[1]\n",
    "    \n",
    "    avg_utterance_embedding = get_average_utterance_embedding(utterance, embed_dim, stop_words, w2i, trained_word_embeddings)\n",
    "    \n",
    "    similar_responses = []  \n",
    "    \n",
    "    all_chat_reps = []    \n",
    "    all_chat_reps.append(avg_utterance_embedding.numpy())\n",
    "    all_chat_indices = [(-1,-1)]\n",
    "    for c in range(len(similar_movie_chat)):\n",
    "        chat = similar_movie_chat[c]\n",
    "        enc = chat.encoder_chat\n",
    "        dec = chat.decoder_chat\n",
    "        \n",
    "        for s in range(len(enc)):\n",
    "            sent = enc[s]            \n",
    "            sent = tknzr.tokenize(sent)\n",
    "            #print(sent)\n",
    "            sent_avg_embedding = get_average_utterance_embedding(sent, embed_dim, stop_words, w2i, trained_word_embeddings)\n",
    "            \n",
    "            all_chat_reps.append(sent_avg_embedding.numpy())\n",
    "            #chat index and then sentence index for speaker 1\n",
    "            #so that we can get the related speaker 2 utterance\n",
    "            all_chat_indices.append((c,s))  \n",
    "        \n",
    "    #print(all_chat_reps[0])\n",
    "    distances, indices = k_nearest_neighbors(n, all_chat_reps)\n",
    "    print(indices)\n",
    "    neighbours = indices[0]\n",
    "    \n",
    "    for n in neighbours:\n",
    "        (c,s) = all_chat_indices[n]\n",
    "        print(c,s)\n",
    "        if c != -1:\n",
    "            print(similar_movie_chat[c].encoder_chat[s])\n",
    "            print(similar_movie_chat[c].decoder_chat[s])\n",
    "            similar_responses.append(tknzr.tokenize(similar_movie_chat[c].decoder_chat[s]))\n",
    "            \n",
    "    return similar_responses\n",
    "\n",
    "movie_id = 'tt0058150'\n",
    "n = 5\n",
    "utterance = ['which', 'is', 'your', 'favourite', 'character'] #speaker 1 utterance\n",
    "model = torch.load('model_movie.pkl', map_location='cpu' )\n",
    "similar_responses = get_similar_movie_responses(movie_id, n, model, w2i_rpc, i2w_rpc, utterance, neighbours,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my',\n",
       "  'favorite',\n",
       "  'character',\n",
       "  'was',\n",
       "  'bond',\n",
       "  'because',\n",
       "  'he',\n",
       "  'is',\n",
       "  'always',\n",
       "  'dynamic'],\n",
       " ['my',\n",
       "  'favorite',\n",
       "  'character',\n",
       "  'was',\n",
       "  'peter',\n",
       "  'sellers',\n",
       "  'version',\n",
       "  'of',\n",
       "  'james',\n",
       "  'bond',\n",
       "  'he',\n",
       "  'along',\n",
       "  'with',\n",
       "  'woody',\n",
       "  'allen',\n",
       "  'gave',\n",
       "  'the',\n",
       "  'funniest',\n",
       "  'performances',\n",
       "  'in',\n",
       "  'the',\n",
       "  'film'],\n",
       " ['my',\n",
       "  'favorite',\n",
       "  'character',\n",
       "  'was',\n",
       "  'peter',\n",
       "  'sellers',\n",
       "  'version',\n",
       "  'of',\n",
       "  'james',\n",
       "  'bond',\n",
       "  'he',\n",
       "  'along',\n",
       "  'with',\n",
       "  'woody',\n",
       "  'allen',\n",
       "  'gave',\n",
       "  'the',\n",
       "  'funniest',\n",
       "  'performances',\n",
       "  'in',\n",
       "  'the',\n",
       "  'film'],\n",
       " ['casino',\n",
       "  'royale',\n",
       "  'is',\n",
       "  'worth',\n",
       "  'seeing',\n",
       "  'particularly',\n",
       "  'if',\n",
       "  'youre',\n",
       "  'a',\n",
       "  'big',\n",
       "  'bond',\n",
       "  'fan',\n",
       "  'or',\n",
       "  'a',\n",
       "  'big',\n",
       "  'fan',\n",
       "  'of',\n",
       "  'any',\n",
       "  'of',\n",
       "  'the',\n",
       "  'cast'],\n",
       " ['pierce', 'brosnan', 'is', 'best', 'bond', 'behind', 'sean', 'connery']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
