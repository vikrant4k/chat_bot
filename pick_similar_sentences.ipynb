{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "with open('w2i_movie_names.json') as f:\n",
    "    w2i_movies = json.load(f)\n",
    "\n",
    "with open('i2w_movie_names.json') as f:\n",
    "    i2w_movies = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('movie_data_separate.pkl', 'rb') as f:\n",
    "    movie_data = pickle.load(f)\n",
    "    \n",
    "with open('neighbours.pkl', 'rb') as f:\n",
    "    neighbours = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('w2i_review_comments_plot.json') as f:\n",
    "        w2i_rpc = json.load(f)\n",
    "with open('i2w_review_comments_plot.json') as f:\n",
    "        i2w_rpc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35195"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i2w_rpc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(k, embeddings):\n",
    "    k += 1\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='kd_tree')\n",
    "    nbrs.fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_utterance_embedding(utterance, embed_dim, stopwords, w2i, trained_word_embeddings):\n",
    "    #obtain the average embedding for the whole utterance using the word embeddings\n",
    "    #learned in the movie embedding training\n",
    "    utterance_embedding = torch.zeros(embed_dim)\n",
    "    \n",
    "    count = 0\n",
    "    for w in utterance:\n",
    "        #skip stop words\n",
    "        if w in stop_words or w in ['<SOS>', '<EOS>']:\n",
    "            pass\n",
    "        elif w in w2i: #word in dictionary\n",
    "            #print('word',w)\n",
    "            #TO DO REMOVE THIS IF\n",
    "            if w2i[w] < trained_word_embeddings.shape[0]:\n",
    "                word_em = trained_word_embeddings[w2i[w]]\n",
    "                utterance_embedding += word_em\n",
    "                count += 1\n",
    "        else:\n",
    "            #print('Unk',w)\n",
    "            word_em = trained_word_embeddings[0] #unk\n",
    "            utterance_embedding += word_em\n",
    "            count += 1\n",
    "            \n",
    "    #print(utterance_embedding, utterance_embedding/count, count)\n",
    "    avg_utterances_embedding = utterance_embedding/count\n",
    "    \n",
    "    return avg_utterances_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt0061452\n",
      "[[ 0 42  6 26 14 12]\n",
      " [37  1 32 16  3 22]\n",
      " [ 2 43 30 46 45  3]\n",
      " [ 3 46 45 34 40 30]\n",
      " [ 4 45 34 44 12  3]\n",
      " [ 5  3 46 40 22 34]\n",
      " [ 0 42  6 26 14 12]\n",
      " [ 7 30 34 45  3 39]\n",
      " [ 8 34 35 39 44 40]\n",
      " [ 9 40 39 44 30 43]\n",
      " [10 21 22 28 11  3]\n",
      " [11 34  3 46 40 30]\n",
      " [12 31  4 15 45  3]\n",
      " [13 40 34 46  3 39]\n",
      " [14 30 39 40 46 25]\n",
      " [15 31 12 45 34 13]\n",
      " [16  3 25 37  1 32]\n",
      " [17 15  5 39 31 45]\n",
      " [18 34 46 13  3 35]\n",
      " [19 23 34 35 13  3]\n",
      " [20 39 34 45 30 44]\n",
      " [21 10 28 22  3  1]\n",
      " [22 28  5  3 44 45]\n",
      " [23 34 44 19 45 40]\n",
      " [24 29 40 13 45 39]\n",
      " [25 46 40 16 44  3]\n",
      " [ 0 42  6 26 14 12]\n",
      " [27 45 12  5 43 31]\n",
      " [28 22  3 46 45 44]\n",
      " [29 43 46 30 45 44]\n",
      " [30 45 46 39  3 40]\n",
      " [31 15 12 45 40 34]\n",
      " [32 37  1 16  3  5]\n",
      " [33 43 45 40  3 35]\n",
      " [34 45 40 39 44 46]\n",
      " [35 43 34 45  8 44]\n",
      " [36 45 40 44 46 34]\n",
      " [37  1 32 16  3 22]\n",
      " [38 30 16 39  3  2]\n",
      " [39 34 45 44 40 30]\n",
      " [40 34 44 45 13 39]\n",
      " [41 35 13 46 39  8]\n",
      " [ 0 42  6 26 14 12]\n",
      " [43 35 45 44 46  4]\n",
      " [44 45 34 40  4 43]\n",
      " [45 44 34  4 46 40]\n",
      " [46  3 45 34 30 25]]\n",
      "-1 -1\n",
      "8 0\n",
      "which is your favourite character in this\n",
      "my favorite character was bond because he is always dynamic\n",
      "1 0\n",
      "which is your favourite character in this\n",
      "my favorite character was peter sellers version of james bond he along with woody allen gave the funniest performances in the film\n",
      "5 0\n",
      "which is your favourite character in this\n",
      "my favorite character was peter sellers version of james bond he along with woody allen gave the funniest performances in the film\n",
      "2 4\n",
      "some of the scenes seem so real i wish i was a spy\n",
      "casino royale is worth seeing particularly if youre a big bond fan or a big fan of any of the cast\n",
      "2 2\n",
      "totally oh by the way who is your favourite bond\n",
      "pierce brosnan is best bond behind sean connery\n"
     ]
    }
   ],
   "source": [
    "def get_similar_movie_responses(movie_id, n, model, w2i, i2w, utterance, neighbours, stop_words):\n",
    "    '''\n",
    "    w2i and i2w are vocabularies for plot-review-comments\n",
    "    utterance is a tokenized sentence of strings\n",
    "    returning a list of tokenized sentences of strings\n",
    "    '''\n",
    "    similar_movie_data = neighbours[movie_id] \n",
    "    similar_movie_id = similar_movie_data.imdb_id\n",
    "    print(similar_movie_id)\n",
    "    similar_movie_chat = similar_movie_data.chat\n",
    "    \n",
    "    trained_word_embeddings = model['model_state_dict']['word_embedding.weight']\n",
    "    #print(trained_word_embeddings.shape)\n",
    "    embed_dim = trained_word_embeddings.shape[1]\n",
    "    \n",
    "    avg_utterance_embedding = get_average_utterance_embedding(utterance, embed_dim, stop_words, w2i, trained_word_embeddings)\n",
    "    \n",
    "    similar_responses = []  \n",
    "    \n",
    "    all_chat_reps = []    \n",
    "    all_chat_reps.append(avg_utterance_embedding.numpy())\n",
    "    all_chat_indices = [(-1,-1)]\n",
    "    for c in range(len(similar_movie_chat)):\n",
    "        chat = similar_movie_chat[c]\n",
    "        enc = chat.encoder_chat\n",
    "        dec = chat.decoder_chat\n",
    "        \n",
    "        for s in range(len(enc)):\n",
    "            sent = enc[s]            \n",
    "            sent = tknzr.tokenize(sent)\n",
    "            #print(sent)\n",
    "            sent_avg_embedding = get_average_utterance_embedding(sent, embed_dim, stop_words, w2i, trained_word_embeddings)\n",
    "            \n",
    "            all_chat_reps.append(sent_avg_embedding.numpy())\n",
    "            #chat index and then sentence index for speaker 1\n",
    "            #so that we can get the related speaker 2 utterance\n",
    "            all_chat_indices.append((c,s))  \n",
    "        \n",
    "    #print(all_chat_reps[0])\n",
    "    distances, indices = k_nearest_neighbors(n, all_chat_reps)\n",
    "    print(indices)\n",
    "    neighbours = indices[0]\n",
    "    \n",
    "    for n in neighbours:\n",
    "        (c,s) = all_chat_indices[n]\n",
    "        print(c,s)\n",
    "        if c != -1:\n",
    "            print(similar_movie_chat[c].encoder_chat[s])\n",
    "            print(similar_movie_chat[c].decoder_chat[s])\n",
    "            similar_responses.append(tknzr.tokenize(similar_movie_chat[c].decoder_chat[s]))\n",
    "            \n",
    "    return similar_responses\n",
    "\n",
    "movie_id = 'tt0058150'\n",
    "n = 5\n",
    "utterance = ['which', 'is', 'your', 'favourite', 'character'] #speaker 1 utterance\n",
    "model = torch.load('model_movie.pkl', map_location='cpu' )\n",
    "similar_responses = get_similar_movie_responses(movie_id, n, model, w2i_rpc, i2w_rpc, utterance, neighbours,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
