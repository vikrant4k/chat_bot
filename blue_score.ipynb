{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU (version 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "\n",
    "    ngram_counts = collections.Counter()\n",
    "\n",
    "    for order in range(1, max_order + 1):\n",
    "\n",
    "        for i in range(0, len(segment) - order + 1):\n",
    "            ngram = tuple(segment[i:i+order])\n",
    "            ngram_counts[ngram] += 1\n",
    "\n",
    "    return ngram_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=2, smooth=False):\n",
    "    matches_by_order = [0] * max_order\n",
    "    possible_matches_by_order = [0] * max_order\n",
    "    reference_length = 0\n",
    "    translation_length = 0\n",
    "\n",
    "    for (references, translation) in zip(reference_corpus, translation_corpus):\n",
    "        reference_length += min(len(r) for r in references)\n",
    "        translation_length += len(translation)\n",
    "\n",
    "        merged_ref_ngram_counts = collections.Counter()\n",
    "        \n",
    "        for reference in references:\n",
    "            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "\n",
    "        #print(\"ref: \", references,\"\\n hypho: \", translation, \"\\n\")\n",
    "        translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "        overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "       \n",
    "        for ngram in overlap:\n",
    "            matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "\n",
    "        for order in range(1, max_order+1):\n",
    "            possible_matches = len(translation) - order + 1\n",
    "\n",
    "            if possible_matches > 0:\n",
    "                possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "\n",
    "\n",
    "    precisions = [0] * max_order\n",
    "\n",
    "    for i in range(0, max_order):\n",
    "\n",
    "        if smooth:\n",
    "            precisions[i] = ((matches_by_order[i] + 1.) /(possible_matches_by_order[i] + 1.))\n",
    "\n",
    "        else:\n",
    "\n",
    "            if possible_matches_by_order[i] > 0:\n",
    "                 precisions[i] = (float(matches_by_order[i]) / possible_matches_by_order[i])\n",
    "\n",
    "            else:\n",
    "                precisions[i] = 0.0\n",
    "\n",
    "    if min(precisions) > 0:\n",
    "        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "        geo_mean = math.exp(p_log_sum)\n",
    "\n",
    "    else:\n",
    "        geo_mean = 0\n",
    "\n",
    "    ratio = float(translation_length) / reference_length\n",
    "\n",
    "    if ratio > 1.0:\n",
    "        bp = 1.\n",
    "        \n",
    "    else:\n",
    "        bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "\n",
    "    bleu = geo_mean * bp\n",
    "\n",
    "    return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('a_side.txt') as f:\n",
    "    lines = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('a_228.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "ff = open(\"a_228_1.txt\", \"w\", encoding=\"utf8\")\n",
    "\n",
    "for line in lines:\n",
    "    if line[:5] != \"Movie\":\n",
    "        ff.write(line)\n",
    "        \n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "chats = lines.split(\"Speaker 1:\")\n",
    "hypothesis = []\n",
    "references = []\n",
    "\n",
    "for chat in chats[1:]:\n",
    "    sentences = chat.split(\"\\n\")\n",
    "    speaker1 = sentences[0]\n",
    "    model = sentences[1][6:]\n",
    "    \n",
    "    \n",
    "    refs = []\n",
    "    for ref in sentences[2:]:\n",
    "        tokens = word_tokenize(ref[10:])\n",
    "        \n",
    "        if tokens != [] and len(tokens) > 1:\n",
    "            refs.append(word_tokenize(ref[10:]))\n",
    "    \n",
    "    if refs != []:\n",
    "        hypothesis.append(word_tokenize(model))\n",
    "        references.append(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4456"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4456"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(references1, hypothesis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1568931348795922 [0.26513035381750466, 0.09284284284284285] 1.0 1.2816229116945108 4296 3352\n"
     ]
    }
   ],
   "source": [
    "print(bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU (version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def blue_score_text(y_actual,y_predicated):\n",
    "        #check length equal\n",
    "        assert len(y_actual) ==  len(y_predicated)\n",
    "        #list of healine .. each headline has words\n",
    "        no_of_news = len(y_actual)\n",
    "        blue_score = 0.0\n",
    "        for i in range(no_of_news-1):\n",
    "            reference = y_actual[i][0]\n",
    "            hypothesis = y_predicated[i]\n",
    "            \n",
    "            #Avoid ZeroDivisionError in blue score\n",
    "            #default weights\n",
    "            weights=(0.25, 0.25, 0.25, 0.25)\n",
    "            min_len_present = min(len(reference),len(hypothesis))\n",
    "            if min_len_present==0:\n",
    "                continue\n",
    "            if min_len_present<4:\n",
    "                weights=[1.0/min_len_present,]*min_len_present\n",
    "   \n",
    "            blue_score = blue_score + sentence_bleu([reference],hypothesis,weights=weights)\n",
    "        \n",
    "        return blue_score/float(no_of_news-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b.eskili\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\b.eskili\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\b.eskili\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0861981428323952\n"
     ]
    }
   ],
   "source": [
    "print(blue_score_text(references,hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(references, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7453559924999299 [0.9444444444444444, 0.5882352941176471] 1.0 1.125 18 16\n"
     ]
    }
   ],
   "source": [
    "print(bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24434280162267477\n",
      "0.22114006156039112\n",
      "0.09784722247358153\n",
      "0.22114006156039112\n",
      "0.22895111517545935\n",
      "0.22895111517545935\n",
      "0.2515569464643777\n",
      "0.10366025971348962\n",
      "0.23247373706329502\n"
     ]
    }
   ],
   "source": [
    "#ROUGE SENTENCE with multi references dividing ourselves \n",
    "from rouge import Rouge \n",
    "\n",
    "# hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
    "\n",
    "# reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# [sentence] [[ref1 ][ref2]]\n",
    "\n",
    "score_average = 0\n",
    "score_p_1_avg = 0\n",
    "score_r_1_avg = 0\n",
    "score_p_2_avg = 0\n",
    "score_r_2_avg = 0\n",
    "score_p_L_avg = 0\n",
    "score_r_L_avg = 0\n",
    "\n",
    "score_f_1_avg = 0\n",
    "score_f_2_avg = 0\n",
    "score_f_L_avg = 0\n",
    "\n",
    "score=0\n",
    "for i in range(0, len(hypothesis)):\n",
    "    hypo = hypothesis[i]\n",
    "    ref_list = references[i]\n",
    "    score_p_1 = 0\n",
    "    score_r_1 = 0\n",
    "    score_p_2 = 0\n",
    "    score_r_2 = 0\n",
    "    score_p_L = 0\n",
    "    score_r_L = 0\n",
    "\n",
    "    for ref in ref_list:\n",
    "\n",
    "\n",
    "        ref_sntc = \" \".join(ref)\n",
    "        hypo_sntc = \" \".join(hypo)\n",
    "\n",
    "        res = rouge.get_scores(hypo_sntc, ref_sntc)\n",
    "        \n",
    "        score_p_1 = max(score_p_1, res[0]['rouge-1']['p'])\n",
    "        score_r_1 = max(score_r_1, res[0]['rouge-1']['r'])\n",
    "        score_p_2 = max(score_p_2, res[0]['rouge-2']['p'])\n",
    "        score_r_2 = max(score_r_1, res[0]['rouge-2']['r'])\n",
    "        score_p_L = max(score_p_L, res[0]['rouge-l']['p'])\n",
    "        score_r_L = max(score_r_L, res[0]['rouge-l']['r'])\n",
    "        \n",
    "        score_f_1 = max(score_p_1, res[0]['rouge-1']['f'])\n",
    "        score_f_2 = max(score_p_2, res[0]['rouge-2']['f'])\n",
    "        score_f_L = max(score_p_L, res[0]['rouge-l']['f'])\n",
    "        \n",
    "        \n",
    "    score_p_1_avg += score_p_1\n",
    "    score_r_1_avg += score_r_1\n",
    "    score_p_2_avg += score_p_2\n",
    "    score_r_2_avg += score_r_2\n",
    "    score_p_L_avg += score_p_L\n",
    "    score_r_L_avg += score_p_L\n",
    "    \n",
    "    score_f_1_avg += score_f_1\n",
    "    score_f_2_avg += score_f_2\n",
    "    score_f_L_avg += score_f_L\n",
    "    \n",
    "    score = score+score_p_1\n",
    "    \n",
    "score_p_1_avg = score_p_1_avg/(len(hypothesis)-1)\n",
    "score_r_1_avg = score_r_1_avg/(len(hypothesis)-1)\n",
    "score_p_2_avg = score_p_2_avg/(len(hypothesis)-1)\n",
    "score_r_2_avg = score_r_2_avg/(len(hypothesis)-1)\n",
    "score_p_L_avg = score_p_L_avg/(len(hypothesis)-1)\n",
    "score_r_L_avg = score_r_L_avg/(len(hypothesis)-1)\n",
    "\n",
    "score_f_1_avg = score_f_1_avg/(len(hypothesis)-1)\n",
    "score_f_2_avg = score_f_2_avg/(len(hypothesis)-1)\n",
    "score_f_L_avg = score_f_L_avg/(len(hypothesis)-1)\n",
    "\n",
    "\n",
    "print(score_p_1_avg)\n",
    "print(score_r_1_avg)\n",
    "print(score_p_2_avg)\n",
    "print(score_r_2_avg)\n",
    "print(score_p_L_avg)\n",
    "print(score_r_L_avg)\n",
    "\n",
    "print(score_f_1_avg)\n",
    "print(score_f_2_avg)\n",
    "print(score_f_L_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9065.169803528886"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
